# A Tour of Optimizers in PyTorch

In this repo we'll be walking through different optimization algorithms by describing how they work and then implementing them in PyTorch.

We'll cover:
- SGD
- SGD with momentum
- Adagrad
- Adadelta
- RMSprop
- Adam

More may be added in the future!

The notebook is best rendered in Jupyter's NBViewer via [this](https://nbviewer.jupyter.org/github/bentrevett/pytorch-optimizers/blob/main/a-tour-of-optimizers.ipynb) link. GitHub does a pretty poor job of rendering equations in notebooks.

If you find any mistakes or have any feedback, please submit an [issue](https://github.com/bentrevett/a-tour-of-optimizers/issues/new) and I'll try and respond ASAP.

### Resources

- https://ruder.io/optimizing-gradient-descent/
- https://mlfromscratch.com/optimizers-explained/#/
- https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/
- https://pytorch.org/docs/stable/optim.html
- https://github.com/pytorch/pytorch/tree/master/torch/optim
